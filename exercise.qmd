---
title: "Practical Bayesian Modeling with Stan and brms"
author: "Masatoshi Katabuchi"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 12pt
format:
  html:
    theme: coderpro
    toc: true
    toc-depth: 2
    number-sections: true
    smooth-scroll: true
    standalone: true
    embed-resources: true
execute:
  cache: true
---

```{r global_options, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  cache = FALSE,
  fig.align = "center",
  fig.show = "hold"
)
```

# Docker (optional)

Run this on your terminal to start an RStudio Server with all the necessary packages installed.

```
docker run -d -p 8787:8787 \
  -e PASSWORD=123456  \
  mattocci/afec-bayes:prod
```

Access `127.0.0.1:8787` and log in with the username `rstudio` and the password `123456` (or your own password).

**Note:**
Because you’re running this container on your own machine and mapping only to 127.0.0.1:8787, the RStudio Server isn’t accessible from outside your computer.
So, using a simple password like `123456` is fine and safe for local development.

# Setup

```{r setup, message=FALSE, cache=FALSE, warning=FALSE}
library(brms)
library(cmdstanr)
library(tidyverse)
library(bayesplot)
library(patchwork)
cmdstanr::set_cmdstan_path("/opt/cmdstan/cmdstan-2.37.0")

my_theme <- theme_bw() + theme(
    axis.text = element_text(size = 14),
    strip.text = element_text(size = 14)
  )

theme_set(my_theme)
```

```{r}
library(tictoc)
```


# Normal model

Leaf mass per are (LMA) is an important trait that reflects plant strategies.
LMA for interspecifc data often shows log-normal distribution.

$$
\text{log}(y_i) \sim N(\mu, \sigma)
$$

**Q:** What are the mean ($\mu$) and standard deviation ($\sigma$) of log LMA?

```{r, echo=FALSE}
set.seed(123)
n <- 100
mu <- 4.6
sigma <- 0.7
y <- rnorm(n, mean = mu, sd = sigma)
exp(y) |> hist(main = "", xlab = "LMA (g/m2)")
```

```{r, echo=FALSE}
y |> hist(main = "", xlab = "log LMA (g/m2)")
```


## Dummy data

```{r}
set.seed(123)
n <- 100
mu <- 4.6
sigma <- 0.7
y <- rnorm(n, mean = mu, sd = sigma)

hist(y)
```

`cmdstan` uses list format for data input.
```{r}
normal_list <- list(
  y = y,
  N = length(y)
)
```

`brms` uses dataframe (tibble) format for data input like `lme4::lmer` and `stats::glm`.
```{r}
normal_df <- tibble(
  y = y,
  N = length(y)
)
```

## CmdStan

We need to write and save stan codes (.stan files).

`stan/normal.stan`

```{r, echo=FALSE, results='asis', cache=FALSE}
cat("```stan\n", paste(readLines("stan/normal.stan"), collapse = "\n"), "\n```")
```

`stan/normal_vague.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/normal_vague.stan"), collapse = "\n"), "\n```")
```

These commands complie the models, translating Stan code to C++ and then into machine-executable files.
This takes a while (about 20 secs to 1 min) the first time only.

```{r}
normal_mod <- cmdstan_model("stan/normal.stan")
normal_vague_mod <- cmdstan_model("stan/normal_vague.stan")
```


```{r}
# Persist CSV outputs to avoid /tmp cleanup during render
normal_out_dir <- file.path("assets", "cmdstan", "normal")
if (!dir.exists(normal_out_dir)) dir.create(normal_out_dir, recursive = TRUE)

normal_fit <- normal_mod$sample(
  data = normal_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000,  # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 500,        # print update every 500 iters
  output_dir = normal_out_dir
)
```

We don't have to re-compile the model when we want to change number iterations, chains, or data.

```{r, eval=FALSE}
normal_vague_fit <- normal_vague_mod$sample(
  data = normal_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500 # print update every 500 iters
)
```

### Summary

We use this output when writing manuscripts and for model diagnostics.

```{r}
normal_fit$summary()
```

- `lp__`: log posterior.
  - $\text{log}\; p(\theta \mid \text{data}) \propto \text{log}\; p(\text{data} \mid \theta) + \text{log}\; p(\theta)$

- `rhat` (Gelman-Rubin statistic): should be close to 1.
  - Measures the convergence of the chains.
  - `rhat` > 1.1: Definitely bad.
  - 1.01 < `rhat` < 1.1: Suspicious.
  - `rhat` <=  1.01: Good.

- `ess_bulk` and `ess_tail`: Effective sample size for bulk and tail of the posterior distribution.
  - `ess_bulk`: sampling efficiency for the bulk posteriors (e.g., mean, SD).
  - `ess_tail`: sampling efficiency for the tails (e.g., quantiles like 2.5%, 97.5%).
  - `ess_bulk`, `ess_tail` > 400: Good.


### Draws (posterior samples)

For each parameter, we have 1000 iterations $\times$ 4 chains = 4000 posteriors.
We use this for visualization and diagnostics.

```{r}
normal_fit$draws()
```


Trace plots for diagnosing convergence and mixing of the chains.
```{r hello}
normal_draws <- as_draws_df(normal_fit$draws())

color_scheme_set("viridis")
mcmc_trace(normal_draws, pars = c("mu", "sigma"))
```

Histograms of the posterior distributions of the parameters.

```{r}
mcmc_hist(normal_draws, pars = c("mu", "sigma"))
```


### Diagnostic summary

```{r}
normal_fit$diagnostic_summary()
```

- `num_divergent`: indicates the number of iterations (sampling transitions) where the Hamiltonian trajectory failed (**divergent transition**).
  - Even 1-2 divergent transitions suggest that model may not be reliable, especially in hierarchical models.
  - We need reparameterization or increase `adapt_delta` to make the sampler take smaller steps.

- `num_max_treedepth`: indicates the number of iterations where there are not enough leapfrog steps.
  - This can indicate that the model is complex or that the priors are too tight.
  - We can increase `max_treedepth` to allow more steps, but this can increase the computation time.

- `ebfmi`: Energy-Bayesian Fraction of Missing Information
  -	Measures whether the resampled momentum generates enough variation in energy to explore the posterior efficiently.
  - Low `ebfmi` means the sampler may be stuck in tight regions and exploring poorly, even if Rhat and ESS look fine.
  - Guidelines:
    - `ebfmi` < 0.3: Bad
    - 0.3 < `ebfmi` <= 1: Acceptable
    - `ebfmi` >= 1: Good

### Methods text example

"We estimated posterior distributions of all parameters using the Hamiltonian Monte Carlo (HMC) algorithm implemented in Stan (Carpenter et al., 2017) with weakly informative priors (Gelman et al., 2008).
The HMC algorithm uses gradient information to propose new states in the Markov chain, leading to a more efficient exploration of the target distribution than traditional Markov chain Monte Carlo (MCMC) methods that rely on random proposals (Carpenter et al., 2017).
This efficiency allows us to achieve convergence with fewer iterations than
traditional MCMC methods.
**Four independent chains** were run for **2000 iterations** for each model with a **warm-up of 1000 iterations**.
Convergence of the posterior distribution was assessed using the **Gelman–Rubin statistic with a convergence threshold of 1.1** (Gelman et al., 2013), ensuring **effective sample sizes greater than 400** (Vehtari et al., 2021), and by **monitoring divergent transitions** (Betancourt, 2016) for all parameters."

## `brms`

`brms` uses an `lme4`-like syntax.

```{r, eval=FALSE}
normal_fit_brm0 <- brm(y ~ 1, family = gaussian(), data = normal_df)
```

We can also specify priors for the parameters.
The prior argument can be a bit tricky, since it is not always obvious which parameter names correspond to which parts of the model, especially when the model becomes more complex.

We can also control the number of warmup, iterations, ...

The `cmdstanr` backend is generally faster than the default `rstan` backend.

```{r normal-brm}
normal_fit_brm <- brm(
  y ~ 1,
  family = gaussian(),
  data = normal_df,
  prior = c(
    prior(normal(0, 5), class = "Intercept"),
    prior(cauchy(0, 2.5), class = "sigma")
  ),
  seed = 123,
  iter = 2000, # total iterations (warmup + post-warmup)
  warmup = 1000,
  chains = 4, cores = 4,
  backend = "cmdstanr"
)
```

### Summary

```{r}
summary(normal_fit_brm)
```

### Draws (posterior samples)

```{r}
normal_draws_brm <- posterior::as_draws_df(normal_fit_brm)
normal_draws_brm
```

```{r}
mcmc_trace(normal_draws_brm, pars = c("b_Intercept", "sigma"))
mcmc_hist(normal_draws_brm, pars = c("b_Intercept", "sigma"))
```

### Diagnostic summary

```{r}
rstan::check_hmc_diagnostics(normal_fit_brm$fit)
```

# Poisson model

Count data (e.g., species richness, seed counts, pollinator visits...) often follows Poisson distribution.

$$
y_i \sim \text{Poisson}(\lambda)
$$

$$
y_i \sim \operatorname{Poisson\_log}(\log \lambda)
$$

**Q:** What are the mean and variance ($\lambda$) of species richnees per plot?


```{r, echo=FALSE}
set.seed(123)
n <- 100
y <- rpois(n, lambda = 12.3)
hist(y, main = "", xlab = "Species richness")
```


## Dummy data

```{r}
set.seed(123)
y <- rpois(n, lambda = 12.3)

pois_list <- list(
  y = y,
  N = n
)

pois_df <- tibble(
  y = y,
  N = n
)
```

<!-- poisson_mod <- cmdstan_model("stan/poisson.stan") -->

## CmdStan

`stan/pois.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/pois.stan"), collapse = "\n"), "\n```")
```

`stan/pois_re.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/pois_re.stan"), collapse = "\n"), "\n```")
```

```{r}
pois_mod <- cmdstan_model("stan/pois.stan")
pois_re_mod <- cmdstan_model("stan/pois_re.stan")
```

```{r}
pois_fit <- pois_mod$sample(
  data = pois_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # print update every 500 iters
)
```

```{r}
pois_re_fit <- pois_re_mod$sample(
  data = pois_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # print update every 500 iters
)
```

```{r}
pois_fit$summary()
```

```{r}
pois_re_fit$summary()
```

## brms

```{r}
pois_fit_brm <- brm(y ~ 1,
  family = poisson(),
  data = pois_df,
  refresh = 0,
  backend = "cmdstanr")
```

```{r}
summary(pois_fit_brm)
```


# Linear model

The simplest multiple linear regression model is the following, with two predictors ($x_1$ and $x_2$), two slopes ($\beta_1$ and $\beta_2$) and intercept coefficient ($\beta_0$), and normally distributed noise ($\sigma$).
This model can be written using standard regression notation as

$$
y_i \sim N(\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}, \sigma)
$$


or

$$
y_i \sim N(\mu_i = \boldsymbol{\beta \cdot x_i}, \sigma)
$$

where $\boldsymbol{\beta}$ is a 1 $\times$ 3 coefficent matrix (or row vector) and $\boldsymbol{x_i}$ is a 3 $\times$ 1 predictor matrix (including intercept).
We use $\boldsymbol{x}$ (3 $\times$ N) to compute the $\mu$ vector with length N.

**Q:**
What are the posterir means and 95% credible intervals of the coefficents ($\beta_0$, $\beta_1$, $\beta_2$)?

```{r}
set.seed(123)
n <- 100
beta <- c(2, 1.2, -0.8)
x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 0, sd = 1)
sigma <- 0.4
y <- rnorm(n, mean = beta[1] + beta[2] * x1 + beta[3] * x2, sd = sigma)
```

```{r}
lm_list <- list(
  y = y,
  N = length(y),
  x = rbind(1, x1, x2) # 3 x N matrix
)

lm_df <- tibble(
  y = y,
  N = length(y),
  x1 = x1,
  x2 = x2
)
```

## CmdStan

`stan/lm.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/lm.stan"), collapse = "\n"), "\n```")
```

```{r lm-model}
lm_mod <- cmdstan_model("stan/lm.stan")

lm_fit <- lm_mod$sample(
  data = lm_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # don't print update
)
```

Use `posterior::summarise_draws` to check more detailed summary statistics.

```{r lm-summary}
lm_summary <- posterior::summarise_draws(
  lm_fit$draws(),
  mean = ~mean(.x),
  sd = ~sd(.x),
  ~posterior::quantile2(.x, probs = c(0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975))
)

lm_summary
```

## brms

```{r lm-brm}
lm_fit_brm <- brm(
  y ~ x1 + x2,
  family = gaussian(),
  data = lm_df,
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 2.5), class = "b"),
    prior(cauchy(0, 2.5), class = "sigma")
  ),
  refresh = 0, # don't print update
  backend = "cmdstanr"
)

summary(lm_fit_brm)
```

```{r lm-summary-brm}
lm_summary_brm <- posterior::summarise_draws(
  posterior::as_draws_df(lm_fit_brm),
  mean = ~mean(.x),
  sd = ~sd(.x),
  ~posterior::quantile2(.x, probs = c(0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975))
)

lm_summary_brm
```

# Varying intercepts

There is a power-law relationship ($y =\beta_0x^{\beta_1}$) between tree diameter (DBH) and tree maximum height, and the scaling factor $\beta_0$ varies among species.

```{r, messagen=FALSE, echo=FALSE, fig.height=4}
set.seed(12345)

n_sp <- 10
n_rep <- 80
trait <- rnorm(n_sp, 0, 1)
gamma0 <- 0.6
gamma1 <- 0.05
b1_hat <- gamma1 * trait + gamma0
b1 <- rnorm(n_sp, b1_hat, 0.01)
b1 <- mean(b1)
b0 <- rnorm(n_sp, 0.55, 0.05)
y_fun <- function(beta0, beta1) beta0 + beta1 * log_xx
log_xx <- seq(1, 100, length = n_rep) |> log()

log_y <- map2(as.list(b0), as.list(b1), y_fun) |> unlist()

tmp <- tibble(y = exp(log_y),
              x = rep(exp(log_xx), n_sp),
              sp = rep(paste0("sp", 1:n_sp), each = n_rep))

tmp2 <- tibble(x = trait, y = b1, sp = rep(paste0("sp", 1:n_sp)))

p1 <- tmp |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_line() +
  xlab("DBH (cm)") +
  ylab(expression("Height (m)")) +
  ggtitle(expression(y == beta[0] * x^beta[1])) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 18)
    )

p2 <- tmp |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_line() +
  xlab("DBH (cm)") +
  ylab(expression("Height (m)")) +
  ggtitle(expression(log(y) == log(beta[0]) + beta[1] * log(x))) +
  scale_x_log10() +
  scale_y_log10() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 16)
    )
p1 + p2
```


$$
\text{log}\; y_{ij} \sim N(\text{log}\; \beta_{0j} + \beta_1 x_{i}, \sigma)
$$

$$
\text{log}\; \beta_{0j} \sim N(\mu_0, \tau)
$$

$$
\mu_0 \sim N(0, 2.5)
$$

$$
\tau \sim \text{Half-Cauchy}(0, 1)
$$

**Q:**
What are the species-level scaling factors ($\beta_{0j}$) and the global scaling exponent ($\beta_1$)?

## dummy data

- We simulate a dataset with 10 species and 20 trees per species.
- Each species has wood density (wd) values.

```{r, message=FALSE, fig.height=6}
set.seed(12345)

n_sp <- 10
n_rep <- 20
wd <- rnorm(n_sp, 0, 1)
gamma0 <- 0.6
gamma1 <- 0.1
sigma_y <- 0.1

b1_hat <- gamma1 * wd + gamma0
b1 <- rnorm(n_sp, b1_hat, 0.01)
log_b0 <- rnorm(n_sp, 0.55, 0.05)

# ---- simulate ----
allo_df <- tibble(
  sp     = factor(rep(paste0("sp", 1:n_sp), each = n_rep)),
  wd  = rep(wd, each = n_rep),
  # now log_xx ~ Normal(mean log‐dbh, sd
  log_xx = rnorm(n_sp * n_rep, mean = log(40), sd = 0.5)) |>
  mutate(
    # add observation‐level noise on log‐height
    log_y = rnorm(
      n(),
      log_b0[as.integer(sp)] + b1[as.integer(sp)] * log_xx,
      sigma_y),
    dbh = exp(log_xx),
    h = exp(log_y)) |>
  select(sp, wd, dbh, h)

dbh_hist <- allo_df |>
  ggplot(aes(dbh)) +
  geom_histogram() +
  xlab("DBH (cm)") +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24)
    )

h_hist <- allo_df |>
  ggplot(aes(h)) +
  geom_histogram() +
  xlab("Height (m)") +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24)
    )

p1 <- allo_df |>
  ggplot(aes(x = dbh, y = h, col = sp)) +
  geom_point() +
  xlab("DBH (cm)") +
  ylab(expression("Height (m)")) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24)
    )

p2 <- p1 +
  scale_x_log10() +
  scale_y_log10()


dbh_hist + h_hist + p1 + p2
```

```{r}
allo_list <- list(
  log_h = log(allo_df$h),
  log_dbh = log(allo_df$dbh),
  sp = as.integer(allo_df$sp),
  N = nrow(allo_df),
  J = allo_df$sp |> unique() |> length()
)
```


## CmdStan

`stan/vint.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/vint.stan"), collapse = "\n"), "\n```")
```

```{r vint-model}
vint_mod <- cmdstan_model("stan/vint.stan")

vint_fit <- vint_mod$sample(
  data = allo_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # don't print update
)

vint_fit$summary()
vint_fit$diagnostic_summary()
```

## brms

```{r vint-brm}
priors <- c(
  prior(normal(0, 2.5), class = Intercept),
  prior(normal(0, 2.5),   class = b),
  prior(cauchy(0, 1),   class = sd),
  prior(cauchy(0, 1),   class = sigma)
)

vint_fit_brm <- brm(
  log(h) ~ log(dbh) + (1 | sp),
  family = gaussian(),
  data = allo_df,
  prior = priors,
  seed = 123,
  refresh = 0, # don't print update
  backend = "cmdstanr"
)

summary(vint_fit_brm)
```
```{r, fig.height=7}
plot(vint_fit_brm)
```

## lme4

```{r vint-lme4}
vint_fit_lme <- lme4::lmer(
  log(h) ~ log(dbh) + (1 | sp),
  data = allo_df)

summary(vint_fit_lme)
```


## Visualization

### CmdStan

```{r vint-cmdstan-vis, fig.height=6}
# Predicted DBH-height curves per species with 95% intervals
pred_grid <- allo_df |>
  dplyr::group_by(sp) |>
  dplyr::summarise(
    dbh = list(seq(min(dbh), max(dbh), length.out = 60)),
    .groups = "drop"
  ) |>
  tidyr::unnest(dbh)

cmd_draws <- posterior::as_draws_matrix(vint_fit$draws(c("beta1", "log_beta0")))
beta1_draws <- cmd_draws[, "beta1"]
log_beta0_cols <- grep("^log_beta0\\[", colnames(cmd_draws), value = TRUE)
species_levels <- levels(allo_df$sp)
```

DBH for predcited lines for each species.
```{r}
pred_grid
```

Posterior draws of the slope parameter ($\beta_1$).

```{r}
str(beta1_draws)
```

Colmun names for log-transformed species-level intercepts ($\log \beta_{0j}$).
We use this to extract posterior draws for each species.

```{r}
str(log_beta0_cols)
```

```{r}
pred_cmd_summary <- dplyr::bind_cols(
  pred_grid,
  purrr::map2_dfr(
    match(pred_grid$sp, species_levels),
    log(pred_grid$dbh),
    ~{
      log_mu <- cmd_draws[, log_beta0_cols[.x]] + beta1_draws * .y
      height_draws <- exp(log_mu)
      tibble::tibble(
        estimate = stats::median(height_draws),
        lower = stats::quantile(height_draws, 0.025),
        upper = stats::quantile(height_draws, 0.975)
      )
    }
  )
)

```

Compute predicted height summary for each species and DBH.

```{r}
pred_cmd_summary
```

```{r}
ggplot(pred_cmd_summary, aes(dbh, estimate, colour = sp)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = sp), alpha = 0.15, colour = NA) +
  geom_line() +
  geom_point(
    data = allo_df,
    aes(dbh, h, colour = sp),
    size = 1,
    alpha = 0.6,
    inherit.aes = FALSE
  ) +
  scale_x_log10() +
  scale_y_log10() +
  guides(fill = "none") +
  labs(
    x = "Diameter at breast height (cm)",
    y = "Height (m)",
    colour = "Species"
  )
```

### brms

```{r, message=FALSE}
pred_summary <- fitted(vint_fit_brm, newdata = pred_grid, re_formula = NULL) |>
  tibble::as_tibble() |>
  dplyr::bind_cols(pred_grid) |>
  dplyr::mutate(
    estimate = exp(Estimate),
    lower = exp(Q2.5),
    upper = exp(Q97.5)
  )

ggplot(pred_summary, aes(dbh, estimate, color = sp)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = sp), alpha = 0.15, colour = NA) +
  geom_line() +
  geom_point(
    data = allo_df,
    aes(dbh, h, colour = sp),
    size = 1,
    alpha = 0.6,
    inherit.aes = FALSE
  ) +
  scale_x_log10() +
  scale_y_log10() +
  guides(fill = "none") +
  labs(
    x = "Diameter at breast height (cm)",
    y = "Height (m)",
    colour = "Species"
  )
```





# Reparameterization for multilevel models

[Diagnosing Biased Inference with Divergences](https://betanalpha.github.io/assets/case_studies/divergences_and_bias.html#3_a_non-centered_eight_schools_implementation) by Michael Betancourt

## Eight schools example

What is the effect of the treatment (a new education method) on students' scores for each school and across schools?

```{r, echo=FALSE, message=FALSE}
# Eight schools estimates and standard errors
y     <- c(28,  8, -3,  7, -1,  1, 18, 12)
sigma <- c(15, 10, 16, 11,  9, 11, 10, 18)

# Number of schools
J <- length(y)

# Pack into a list for Stan
schools_dat <- list(
  J     = J,
  y     = y,
  sigma = sigma
)

# (Optionally) also create a data frame for quick inspection / plotting
schools_df <- tibble(
  school = paste0("School_", 1:J),
  y      = y,
  sigma  = sigma
)

# Inspect
# print(shools_dt)
schools_df |> kableExtra::kbl()
```


## Centered parameterization

$$
y_j \sim \mathcal{N}\bigl(\theta_j,\;\sigma_j\bigr)
$$

$$
\theta_j \sim \mathcal{N}\bigl(\mu,\;\tau \bigr)
$$

$$
\mu \sim \mathcal{N}\bigl(0,\;5 \bigr)
$$

$$
\tau \sim \text{Half-Cauchy}(0, 2.5)
$$

*Note: $\sigma_j$ is known constant (i.e., data), we don't have to estimate it.*

This parameterization is intuitive, but it often leads to convergence issues in hierarchical models.

## Non-centered parameterization

$$
\tilde{\theta_j} \sim \mathcal{N}\bigl(0,\;1 \bigr)
$$

$$
\theta_j = \mu + \tau \cdot \tilde{\theta_j}
$$

In this parameterization, we introduce a new latent variable $\tilde{\theta_j}$ that is independent of the other parameters.
This allows the sampler to explore the posterior more efficiently and avoids problems with convergence.
We often use this parameterization when we have hierarchical models with varying intercepts or slopes.

# Varying slopes and intercepts

There is a power-law relationship ($y =\beta_0 x^{\beta_1}$) between tree diameter (DBH) and tree maximum height, and both the scaling factor $\beta_0$ and the expoent $\beta_1$ vary among species.

$$
\text{log}\; y_{ij} \sim N(\text{log}\; \beta_{0j} + \beta_{1j} x_{ij}, \sigma)
$$

$$
\begin{bmatrix}
\text{log}\; \beta_{0j} \\
\beta_{1j}
\end{bmatrix}
\sim
\mathrm{MVN}
\Biggl(
\begin{bmatrix}
\mu_{0} \\
\mu_{1}
\end{bmatrix},
\begin{bmatrix}
\sigma_{0}^2  & \rho \sigma_{0} \sigma_{1} \\
\rho \sigma_{0} \sigma_{1} & \sigma_{1}^2
\end{bmatrix}
\Biggr)
$$

## Centered parameterization


$$
\boldsymbol\mu \sim N(0, 2.5)
$$

$$
\boldsymbol\Sigma \sim \mathcal{W}^{-1}_p(\nu,\,\Psi)
$$

This parameterization using inverse-whishart distribution $\mathcal{W}^{-1}$ is **not recommended** for hierarchical models with varying slopes and intercepts.

## Non-Centered parameterization

[Optimization through Cholesky factorization](https://mc-stan.org/docs/stan-users-guide/regression.html#hierarchical-priors.section)

$$
\boldsymbol z_j \sim \mathcal{N}(\mathbf0,\,I_2)
$$
$$
\begin{pmatrix}\log\beta_{0j}\\[3pt]\beta_{1j}\end{pmatrix}
  = \boldsymbol\mu + L\, \boldsymbol z_j
$$


Some linear algebra (Cholesk decomposition):

$$
\Sigma \;=\;L\,L^\top
\;=\;\bigl[\mathrm{diag}(\tau)\,L_\Omega\bigr]\,
\bigl[L_\Omega^\top\,\mathrm{diag}(\tau)\bigr]
\;=\;\mathrm{diag}(\tau)\,\Omega\,\mathrm{diag}(\tau).
$$

$$
L_\Omega \sim \mathrm{LKJ\_Corr\_Cholesky}(2) \quad \text{or}  \quad \Omega \sim \mathrm{LKJ\_Corr}(2)
$$
$$
\tau_i \sim \mathrm{Half\text{-}Cauchy}(0,1)
$$


Rather than sampling the correlation matix $\Omega$ directly, sampling the Cholesky factor $L_\Omega$ is computationally more efficient and numerically stable.

In the LKJ(\eta) prior:

- $\eta$ = 1 indicate flat prior (uniform distribution) over correlation matrices.

- $\eta$  > 1 indicate weaker correlations.

We usually use $\eta$  = 2 because we assume at least weak correlations among the parameters, but still allow moderate to strong correlations.

## CmdStan

`stan/vslope.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/vslope.stan"), collapse = "\n"), "\n```")
```

```{r}
allo_vslope_list <- list(
  y = log(allo_df$h),
  x = cbind(1, log(allo_df$dbh)),
  jj = as.integer(allo_df$sp),
  N = nrow(allo_df),
  K = 2, # number of predictors (intercept + slope)
  J = allo_df$sp |> unique() |> length(),
  L = 1 # number of group-level predictors (intercept)
)
allo_vslope_list$u <- matrix(1, nrow = allo_vslope_list$L, ncol = allo_vslope_list$J)
```

```{r}
vslope_mod <- cmdstan_model("stan/vslope.stan")

vslope_fit <- vslope_mod$sample(
  data = allo_vslope_list,
  seed = 1234,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  adapt_delta = 0.95, # increase adapt_delta to avoid divergent transitions
  max_treedepth = 15, # increase max_treedepth to avoid max treedepth errors
  refresh = 0, # don't print update
  show_messages = FALSE   # suppress Stan’s “Informational Message” output
)

vslope_fit$summary()
vslope_fit$diagnostic_summary()
```

## brms

```{r vslope-brm}
priors <- c(
  prior(normal(0, 2.5), class = Intercept),
  prior(normal(0, 2.5),   class = b),
  prior(lkj(2), class = cor),                # Ω ~ LKJ(2)
  prior(cauchy(0, 1),   class = sigma)
)

tic()
slope_fit_brm <- brm(
  log(h) ~ log(dbh) + (1 + log(dbh) | sp),
  family = gaussian(),
  data = allo_df,
  prior = priors,
  refresh = 0, # don't print update
  control = list(
    adapt_delta  = 0.95,  # default is 0.8
    max_treedepth = 15    # default is 10
  ),
  backend = "cmdstanr"
)
toc()
```

**Divergent transitions!**
We need to increase `adapt_delta` and `max_treedepth` to avoid divergent transitions.
We may also need to change the priors.
Reparameterization is less  straightforward in `brms` than writing your own Stan code.

```{r}
summary(slope_fit_brm)
```


## lme4

```{r slope-lme4}
slope_fit_lme <- lme4::lmer(
  log(h) ~ log(dbh) + (1 + log(dbh) | sp),
  data = allo_df)
```

**Not convergent!**

```{r}
summary(slope_fit_lme)
```

# Varying slopes and intercepts, and group-level predictors

There is a power-law relationship ($y =\beta_0 x^{\beta_1}$) between tree diameter (DBH) and tree maximum height, and both the scaling factor $\beta_0$ and the expoent $\beta_1$ vary among species.
Those species variation can be explained by wood density (group-level predictor: $u$) of each species.

$$
\text{log}\; y_{ij} \sim N(\text{log}\; \beta_{0j} + \beta_{1j} x_{ij}, \sigma)
$$

$$
\begin{bmatrix}
\text{log}\; \beta_{0j} \\
\beta_{1j}
\end{bmatrix}
\sim
\mathrm{MVN}
\Biggl(
\underbrace{
\begin{bmatrix}
\gamma_{0}^{(\beta_0)} + \gamma_{1}^{(\beta_0)}u_{j}  \\
\gamma_{0}^{(\beta_1)} + \gamma_{1}^{(\beta_1)}u_{j}
\end{bmatrix}}_{\text{mean depends on }u_j},
\begin{bmatrix}
\sigma_{0}^2  & \rho \sigma_{0} \sigma_{1} \\
\rho \sigma_{0} \sigma_{1} & \sigma_{1}^2
\end{bmatrix}
\Biggr)
$$

The rest of part is the same as the section Varying slopes and intercepts.

```{r, eval=FALSE, echo=FALSE}
p3 <- allo_df |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_smooth(method = "lm", se = TRUE, lty = 2,
    alpha = 0.2,
    linewidth = 0.5, col = "black") +
  geom_point(size = 3) +
  xlab("Wood density") +
  ylab("b") +
  theme(legend.position = "none")
```


```{r hoge}
allo_vslope_list$L <- 2 # number of group-level predictors (intercept and wd)
allo_vslope_list$u <- matrix(1, nrow = allo_vslope_list$L, ncol = allo_vslope_list$J)
```

```{r vgrp-cmstan, eval=FALSE}
vgrp_fit <- vslope_mod$sample(
  data = allo_vslope_list,
  seed = 1234,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  adapt_delta = 0.95, # increase adapt_delta to avoid divergent transitions
  max_treedepth = 15, # increase max_treedepth to avoid max treedepth errors
  refresh = 0 # don't print update
)

vgrp_fit$summary()
vgrp_fit$diagnostic_summary()
```

## brms

```{r vgrp-brm, eval=TRUE}
priors <- c(
  prior(normal(0, 2.5), class = Intercept),
  prior(normal(0, 2.5),   class = b),
  prior(lkj(2), class = cor),                # Ω ~ LKJ(2)
  prior(cauchy(0, 1),   class = sigma)
)
vgrp_fit_brm <- brm(
  log(h) ~ log(dbh) * wd + (1 + log(dbh) | sp),
  family = gaussian(),
  data = allo_df,
  prior = priors,
  refresh = 0, # don't print update
  control = list(
    adapt_delta  = 0.95,  # default is 0.8
    max_treedepth = 15    # default is 10
  ),
  backend = "cmdstanr"
)
```

```{r, eval=TRUE}
summary(vgrp_fit_brm)
```

## lme4

```{r group-lme4}
grp_fit_lme <- lme4::lmer(
  log(h) ~ log(dbh) * wd + (1 + log(dbh) | sp),
  data = allo_df)
```


# References

- Betancourt, M. (2017). [Diagnosing Biased Inference with Divergences](https://betanalpha.github.io/assets/case_studies/divergences_and_bias.html#3_a_non-centered_eight_schools_implementation).

- [Gelman, A. et al. Bayesian Data Analysis, Third Edition. (Chapman & Hall/CRC, 2013)](http://www.stat.columbia.edu/~gelman/book/)

- [Stan User's Guide](https://mc-stan.org/docs/stan-users-guide/index.html)

- [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/index.html)


```{r}
devtools::session_info()
```
